{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdc761f5-d708-412b-b7ae-17ca54505ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Challenger model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63f6cd6e-824a-41d5-af1f-4c6c7db80816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 01. Fetch Model information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1447bf-b1bb-4184-9d2d-310b01c272bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register Parameters\n",
    "catalog = \"workspace\"\n",
    "db = \"customer_churn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4311a073-0289-41e1-b569-d70ea0192486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Fully qualified model name\n",
    "model_name = f\"{catalog}.{db}.advanced_mlops_churn\"\n",
    "\n",
    "# We are interested in validating the Challenger model\n",
    "model_alias = \"Challenger\"\n",
    "\n",
    "client = MlflowClient()\n",
    "model_details = client.get_model_version_by_alias(model_name, model_alias)\n",
    "model_version = int(model_details.version)\n",
    "run_info = client.get_run(run_id=model_details.run_id)\n",
    "\n",
    "print(f\"Validating {model_alias} model for {model_name} on model version {model_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2dbc3e4-4cc4-471f-867b-c8ecd0b76654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 02. Model checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df380b64-0eb6-4a0a-9d04-243b26673b74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Description check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40af835b-2823-4894-82c5-9d449bc81506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If there's no description or an insufficient number of characters, tag accordingly\n",
    "if not model_details.description:\n",
    "  has_description = False\n",
    "  print(\"Please add model description\")\n",
    "\n",
    "elif not len(model_details.description) > 20:\n",
    "  has_description = False\n",
    "  print(\"Please add detailed model description (40 char min).\")\n",
    "  \n",
    "else:\n",
    "  has_description = True\n",
    "\n",
    "print(f'Model {model_name} version {model_details.version} has description: {has_description}')\n",
    "client.set_model_version_tag(name=model_name, version=str(model_details.version), key=\"has_description\", value=has_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86ddd9f6-cf39-4160-b658-c6c539073052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Validate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bdcbfe4-57cb-4337-af4d-bf2ae7fd6660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from pyspark.sql.types import StructType\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# Load model as a Spark UDF\n",
    "model_uri = f\"models:/{model_name}@{model_alias}\"\n",
    "label_col = \"churn\"\n",
    "\n",
    "# Predict on a Spark DataFrame\n",
    "try:\n",
    "  # Read labels and IDs\n",
    "  labelsDF = spark.read.table(\"advanced_churn_label_table\")\n",
    "\n",
    "  # Batch score\n",
    "  features_w_preds = fe.score_batch(df=labelsDF, model_uri=model_uri, result_type=labelsDF.schema[label_col].dataType)\n",
    "  display(features_w_preds)\n",
    "  client.set_model_version_tag(name=model_name, version=str(model_version), key=\"predicts\", value=True)\n",
    "\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "  features_w_preds = spark.createDataFrame([], StructType([]))\n",
    "  print(\"Unable to predict on features.\")\n",
    "  client.set_model_version_tag(name=model_name, version=str(model_version), key=\"predicts\", value=False)\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6934ec03-dd16-4fb7-b022-2f328b4498f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Artifact check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf6d14d-15ba-42af-8894-bb522693bf27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "\n",
    "\n",
    "# Create local directory\n",
    "local_dir = \"/tmp/model_artifacts\"\n",
    "if not os.path.exists(local_dir):\n",
    "    os.mkdir(local_dir)\n",
    "\n",
    "# Download artifacts from tracking server - no need to specify DBFS path here\n",
    "local_path = mlflow.artifacts.download_artifacts(run_id=run_info.info.run_id, dst_path=local_dir)\n",
    "\n",
    "# Tag model version as possessing artifacts or not\n",
    "if not os.listdir(local_path):\n",
    "  client.set_model_version_tag(name=model_name, version=model_version, key=\"has_artifacts\", value=False)\n",
    "  print(\"There are no artifacts associated with this model.  Please include some data visualization or data profiling.  MLflow supports HTML, .png, and more.\")\n",
    "\n",
    "else:\n",
    "  client.set_model_version_tag(name=model_name, version=str(model_version), key = \"has_artifacts\", value = True)\n",
    "  print(\"Artifacts downloaded in: {}\".format(local_path))\n",
    "  print(\"Artifacts: {}\".format(os.listdir(local_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df3727df-dfd9-45e2-8150-057a1c8c8784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Model performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b730c5e3-1539-4e68-af86-39812f4f0a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_run_id = model_details.run_id\n",
    "f1_score = mlflow.get_run(model_run_id).data.metrics['test_f1_score']\n",
    "\n",
    "try:\n",
    "    #Compare the challenger f1 score to the existing champion if it exists\n",
    "    champion_model = client.get_model_version_by_alias(model_name, \"Champion\")\n",
    "    champion_f1 = mlflow.get_run(champion_model.run_id).data.metrics['test_f1_score']\n",
    "    print(f'Champion f1 score: {champion_f1}. Challenger f1 score: {f1_score}.')\n",
    "    metric_f1_passed = f1_score >= champion_f1\n",
    "\n",
    "except:\n",
    "    print(f\"No Champion found. Accept the model as it's the first one.\")\n",
    "    metric_f1_passed = True\n",
    "\n",
    "print(f'Model {model_name} version {model_details.version} metric_f1_passed: {metric_f1_passed}')\n",
    "\n",
    "# Tag that F1 metric check has passed\n",
    "client.set_model_version_tag(name=model_name, version=model_details.version, key=\"metric_f1_passed\", value=metric_f1_passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c0bbef5-db14-4319-9f0b-af4f4c021dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 03. Benchmark or business metrics on the eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b1cf347-99fe-4d9e-8d59-bacecde35cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "# Get our validation dataset:\n",
    "validation_df = spark.table('advanced_churn_label_table').filter(\"split='validate'\")\n",
    "\n",
    "# Call the model with the given alias and return the prediction\n",
    "def predict_churn(validation_df, model_alias):\n",
    "    features_w_preds = fe.score_batch(df=validation_df, model_uri=f\"models:/{model_name}@{model_alias}\", \n",
    "                                      result_type=validation_df.schema[label_col].dataType)\n",
    "\n",
    "    return features_w_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b606fb-42c4-4c84-b703-5e4d51bae8ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Note: This is over-simplified and depends on the use-case, but the idea is to evaluate our model against business metrics\n",
    "cost_of_customer_churn = 2000 #in dollar\n",
    "cost_of_discount = 500 #in dollar\n",
    "\n",
    "cost_true_negative = 0 #did not churn, we did not give him the discount\n",
    "cost_false_negative = cost_of_customer_churn #did churn, we lost the customer\n",
    "cost_true_positive = cost_of_customer_churn -cost_of_discount #We avoided churn with the discount\n",
    "cost_false_positive = -cost_of_discount #doesn't churn, we gave the discount for free\n",
    "\n",
    "def get_model_value_in_dollar(model_alias):\n",
    "    # Convert preds_df to Pandas DataFrame\n",
    "    model_predictions = predict_churn(validation_df, model_alias).toPandas()\n",
    "    # Calculate the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(model_predictions['churn'], model_predictions['prediction']).ravel()\n",
    "    return tn * cost_true_negative+ fp * cost_false_positive + fn * cost_false_negative + tp * cost_true_positive\n",
    "\n",
    "try:\n",
    "    champion_model = client.get_model_version_by_alias(model_name, \"Champion\")\n",
    "    champion_potential_revenue_gain = get_model_value_in_dollar(\"Champion\")\n",
    "    challenger_potential_revenue_gain = get_model_value_in_dollar(\"Challenger\")\n",
    "\n",
    "    data = {'Model Alias': ['Challenger', 'Champion'],\n",
    "            'Potential Revenue Gain': [challenger_potential_revenue_gain, champion_potential_revenue_gain]}\n",
    "\n",
    "except:\n",
    "    print(\"No Champion found. Skipping business metrics evaluation.\")\n",
    "    print(\"You can return to re-run this cell after promoting the Challenger model as Champion in the rest of this notebook.\")\n",
    "\n",
    "    data = {'Model Alias': ['Challenger', 'Champion'],\n",
    "            'Potential Revenue Gain': [0, 0]}\n",
    "\n",
    "# Create a bar plot using plotly express\n",
    "px.bar(data, x='Model Alias', y='Potential Revenue Gain', color='Model Alias',\n",
    "    labels={'Potential Revenue Gain': 'Revenue Impacted'},\n",
    "    title='Business Metrics - Revenue Impacted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa236b78-41e6-4e46-a929-a03ef1194a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = client.get_model_version(model_name, model_version)\n",
    "results.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5a1d98a-fc60-41d1-8e4b-c3045cbab0eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 04. Promoting to the Challenger to Champion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47fc6b16-788f-4402-bea4-89b31119c822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if results.tags[\"has_description\"] and results.tags[\"metric_f1_passed\"] and results.tags['predicts']:\n",
    "  print(f\"Registering model {model_name} Version {model_version} as Champion!\")\n",
    "  client.set_registered_model_alias(\n",
    "    name=model_name,\n",
    "    alias=\"Champion\",\n",
    "    version=model_version\n",
    "  )\n",
    "\n",
    "else:\n",
    "  raise Exception(\"Model not ready for promotion\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-challenger-validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
